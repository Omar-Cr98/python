{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk as nk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/omaralamri/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\" Welcome readers. I hope you find it interesting. Please do reply.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Welcome readers.', 'I hope you find it interesting.', 'Please do reply.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize \n",
    "print(sent_tokenize(text))\n",
    "\n",
    "#sent_tokenize يفصل الجمل بناء على النقطة فقط"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Welcome readers.', 'I hope you find it interesting.', 'Please do reply.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "text=\" Welcome readers. I hope you find it interesting. Please do reply.\"\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['مرحبا بكم.', 'نحن نتعلم اساسيات مبادئ استرجاع المعلومات.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Arabic_text=\"مرحبا بكم. نحن نتعلم اساسيات مبادئ استرجاع المعلومات.\"\n",
    "tokenizer.tokenize(Arabic_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome', 'readers', '.', 'I', 'hope', 'you', 'find', 'it', 'interesting', '.', 'Please', 'do', 'reply', '»', '..']\n"
     ]
    }
   ],
   "source": [
    "text = nltk.word_tokenize(\"Welcome readers. I hope you find it interesting. Please do reply»..\")\n",
    "print (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please write a textmy name is omar\n",
      "['my', 'name', 'is', 'omar']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Arabic=input(\"Please write a text\")\n",
    "text = nltk.word_tokenize(Arabic)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Don',\n",
       " 't',\n",
       " 'hesitate',\n",
       " 'to',\n",
       " 'ask',\n",
       " 'questions',\n",
       " 'or',\n",
       " 'send',\n",
       " 'to',\n",
       " 'me',\n",
       " 'your',\n",
       " 'question',\n",
       " 'to',\n",
       " 'mohsarem',\n",
       " 'gmail',\n",
       " 'com']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer=RegexpTokenizer(\"[\\w]+\")\n",
    "tokenizer.tokenize(\"Don't hesitate to ask questions or send to me your question to mohsarem@gmail.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mohsarem@gmail.com']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer=RegexpTokenizer(\"\\S+@\\S+\")\n",
    "tokenizer.tokenize(\"Don't hesitate to ask questions or send to me your question to mohsarem@gmail.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['It', 'is', 'a', 'pleasant', 'evening', '.'], ['Guests', ',', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Food', 'was', 'tasty', '.']]\n"
     ]
    }
   ],
   "source": [
    "text=[\" It is a pleasant evening.\",\"Guests, who came from US arrived at the venue\",\"Food was tasty.\"]\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_docs=[word_tokenize(doc)\n",
    "                for doc in text]\n",
    "print(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['It']]\n",
      " IT IS A PLEASANT EVENING.\n",
      "[['It', 'is'], ['It', 'is']]\n",
      " IT IS A PLEASANT EVENING.\n",
      "[['It', 'is', 'a'], ['It', 'is', 'a'], ['It', 'is', 'a']]\n",
      " IT IS A PLEASANT EVENING.\n",
      "[['It', 'is', 'a', 'pleasant'], ['It', 'is', 'a', 'pleasant'], ['It', 'is', 'a', 'pleasant'], ['It', 'is', 'a', 'pleasant']]\n",
      " IT IS A PLEASANT EVENING.\n",
      "[['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening']]\n",
      " IT IS A PLEASANT EVENING.\n",
      "[['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['Guests']]\n",
      " IT IS A PLEASANT EVENING.\n",
      "[['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['Guests', 'who'], ['Guests', 'who']]\n",
      " IT IS A PLEASANT EVENING.\n",
      "[['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['Guests', 'who', 'came'], ['Guests', 'who', 'came'], ['Guests', 'who', 'came']]\n",
      " IT IS A PLEASANT EVENING.\n",
      "[['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['Guests', 'who', 'came', 'from'], ['Guests', 'who', 'came', 'from'], ['Guests', 'who', 'came', 'from'], ['Guests', 'who', 'came', 'from']]\n",
      " IT IS A PLEASANT EVENING.\n",
      "[['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['Guests', 'who', 'came', 'from', 'US'], ['Guests', 'who', 'came', 'from', 'US'], ['Guests', 'who', 'came', 'from', 'US'], ['Guests', 'who', 'came', 'from', 'US'], ['Guests', 'who', 'came', 'from', 'US']]\n",
      " IT IS A PLEASANT EVENING.\n",
      "[['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['Guests', 'who', 'came', 'from', 'US', 'arrived'], ['Guests', 'who', 'came', 'from', 'US', 'arrived'], ['Guests', 'who', 'came', 'from', 'US', 'arrived'], ['Guests', 'who', 'came', 'from', 'US', 'arrived'], ['Guests', 'who', 'came', 'from', 'US', 'arrived'], ['Guests', 'who', 'came', 'from', 'US', 'arrived']]\n",
      " IT IS A PLEASANT EVENING.\n",
      "[['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at']]\n",
      " IT IS A PLEASANT EVENING.\n",
      "[['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the']]\n",
      " IT IS A PLEASANT EVENING.\n",
      "[['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue']]\n",
      " IT IS A PLEASANT EVENING.\n",
      "[['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Food']]\n",
      " IT IS A PLEASANT EVENING.\n",
      "[['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Food', 'was'], ['Food', 'was']]\n",
      " IT IS A PLEASANT EVENING.\n",
      "[['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['It', 'is', 'a', 'pleasant', 'evening'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Food', 'was', 'tasty'], ['Food', 'was', 'tasty'], ['Food', 'was', 'tasty']]\n",
      " IT IS A PLEASANT EVENING.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "x=re.compile('[%s]' % re.escape(string.punctuation))\n",
    "tokenized_docs_no_punctuation = []\n",
    "for review in tokenized_docs: \n",
    "    new_review = [] \n",
    "    for token in review: \n",
    "        new_token = x.sub(u'', token)\n",
    "        if not new_token == u'': \n",
    "            new_review.append(new_token)\n",
    "            tokenized_docs_no_punctuation.append(new_review)\n",
    "            print(tokenized_docs_no_punctuation)\n",
    "            print(text[0].upper())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/omaralamri/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/omaralamri/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Don't\", 'hesitate', 'ask', 'questions']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stops=set(stopwords.words('english'))\n",
    "words=[\"Don't\",'hesitate','to','ask','questions']\n",
    "[word for word in words if word not in stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['إذ', 'إذا', 'إذما', 'إذن', 'أف', 'أقل', 'أكثر', 'ألا', 'إلا', 'التي', 'الذي', 'الذين', 'اللاتي', 'اللائي', 'اللتان', 'اللتيا', 'اللتين', 'اللذان', 'اللذين', 'اللواتي', 'إلى', 'إليك', 'إليكم', 'إليكما', 'إليكن', 'أم', 'أما', 'أما', 'إما', 'أن', 'إن', 'إنا', 'أنا', 'أنت', 'أنتم', 'أنتما', 'أنتن', 'إنما', 'إنه', 'أنى', 'أنى', 'آه', 'آها', 'أو', 'أولاء', 'أولئك', 'أوه', 'آي', 'أي', 'أيها', 'إي', 'أين', 'أين', 'أينما', 'إيه', 'بخ', 'بس', 'بعد', 'بعض', 'بك', 'بكم', 'بكم', 'بكما', 'بكن', 'بل', 'بلى', 'بما', 'بماذا', 'بمن', 'بنا', 'به', 'بها', 'بهم', 'بهما', 'بهن', 'بي', 'بين', 'بيد', 'تلك', 'تلكم', 'تلكما', 'ته', 'تي', 'تين', 'تينك', 'ثم', 'ثمة', 'حاشا', 'حبذا', 'حتى', 'حيث', 'حيثما', 'حين', 'خلا', 'دون', 'ذا', 'ذات', 'ذاك', 'ذان', 'ذانك', 'ذلك', 'ذلكم', 'ذلكما', 'ذلكن', 'ذه', 'ذو', 'ذوا', 'ذواتا', 'ذواتي', 'ذي', 'ذين', 'ذينك', 'ريث', 'سوف', 'سوى', 'شتان', 'عدا', 'عسى', 'عل', 'على', 'عليك', 'عليه', 'عما', 'عن', 'عند', 'غير', 'فإذا', 'فإن', 'فلا', 'فمن', 'في', 'فيم', 'فيما', 'فيه', 'فيها', 'قد', 'كأن', 'كأنما', 'كأي', 'كأين', 'كذا', 'كذلك', 'كل', 'كلا', 'كلاهما', 'كلتا', 'كلما', 'كليكما', 'كليهما', 'كم', 'كم', 'كما', 'كي', 'كيت', 'كيف', 'كيفما', 'لا', 'لاسيما', 'لدى', 'لست', 'لستم', 'لستما', 'لستن', 'لسن', 'لسنا', 'لعل', 'لك', 'لكم', 'لكما', 'لكن', 'لكنما', 'لكي', 'لكيلا', 'لم', 'لما', 'لن', 'لنا', 'له', 'لها', 'لهم', 'لهما', 'لهن', 'لو', 'لولا', 'لوما', 'لي', 'لئن', 'ليت', 'ليس', 'ليسا', 'ليست', 'ليستا', 'ليسوا', 'ما', 'ماذا', 'متى', 'مذ', 'مع', 'مما', 'ممن', 'من', 'منه', 'منها', 'منذ', 'مه', 'مهما', 'نحن', 'نحو', 'نعم', 'ها', 'هاتان', 'هاته', 'هاتي', 'هاتين', 'هاك', 'هاهنا', 'هذا', 'هذان', 'هذه', 'هذي', 'هذين', 'هكذا', 'هل', 'هلا', 'هم', 'هما', 'هن', 'هنا', 'هناك', 'هنالك', 'هو', 'هؤلاء', 'هي', 'هيا', 'هيت', 'هيهات', 'والذي', 'والذين', 'وإذ', 'وإذا', 'وإن', 'ولا', 'ولكن', 'ولو', 'وما', 'ومن', 'وهو', 'يا']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('arabic'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
